{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65496cfd-f501-4aa9-9bb2-acdaa44644ed",
   "metadata": {},
   "source": [
    "# Strands Agents Fundamentals - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178e3d90-08e3-4ffa-8c5a-16c309270c87",
   "metadata": {},
   "source": [
    "### 1.f Realtime Streaming and handling of Agent Responses - Async Iterators\n",
    "\n",
    "Strands Agents SDK provides support for **[asynchronous iterators](https://strandsagents.com/latest/user-guide/concepts/streaming/async-iterators/)** through the stream_async method, enabling real-time streaming of agent responses in asynchronous environments like web servers, APIs, and other async applications.\n",
    "\n",
    "**Async Iterator event types -** \n",
    "- Text generation from the model\n",
    "- Tool selection and execution\n",
    "- Reasoning process\n",
    "- Errors and completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb1238-a8ec-4ca6-9ecb-976ccca16e57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "import asyncio\n",
    "from strands_tools import calculator\n",
    "\n",
    "async_iter_agent = Agent(\n",
    "    tools=[calculator],\n",
    "    callback_handler=None  # Disable default callback handler\n",
    ")\n",
    "\n",
    "# Async function that iterates over streamed agent events\n",
    "async def process_streaming_response():\n",
    "    query = \"What is 25 * 48 and explain the calculation\"\n",
    "\n",
    "    # Get an async iterator for the agent's response stream\n",
    "    agent_stream = async_iter_agent.stream_async(query)\n",
    "\n",
    "    # Process events as they arrive\n",
    "    async for event in agent_stream:\n",
    "        # print(event)  # OPTIONAL:: uncomment this print statement to display detailed event trace\n",
    "        if \"data\" in event:\n",
    "            # Print text chunks as they're generated\n",
    "            print(event[\"data\"], end=\"\", flush=True)\n",
    "        elif \"current_tool_use\" in event and event[\"current_tool_use\"].get(\"name\"):\n",
    "            # Print tool usage information\n",
    "            print(f\"\\n[Tool use delta for: {event['current_tool_use']['name']}]\")\n",
    "\n",
    "## Run the agent with the async event processing ##\n",
    "# Use below code to invoke agent from Jupyter notebook. \n",
    "# Comment this await statement and uncomment aysncio.run(), if you are not executing this script in a Jupyter notebook.\n",
    "await process_streaming_response()\n",
    "\n",
    "# Uncomment below statement if using in a python file. asyncio.run () fails in Jupyter notebook. \n",
    "# asyncio.run(process_streaming_response()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51abc856-7ae5-4b6f-a58d-33e28149abd2",
   "metadata": {},
   "source": [
    "### 1.g Realtime Streaming and handling of Agent Responses - Callback Handlers\n",
    "\n",
    "**[Callback handlers](https://strandsagents.com/latest/user-guide/concepts/streaming/callback-handlers/)** are a powerful feature of the Strands Agents SDK that allow you to intercept and process events as they happen during agent execution. This enables real-time monitoring, custom output formatting, and integration with external systems.\n",
    "Callback handlers are an alternate for Async Iterators\n",
    "\n",
    "**Callback handlers receive events in real-time as they occur during an agent's lifecycle:**\n",
    "- Text generation from the model\n",
    "- Tool selection and execution\n",
    "- Reasoning process\n",
    "- Errors and completions\n",
    "\n",
    "---\n",
    "\n",
    "#### An Example for Event Loop Lifecycle Tracking\n",
    "\n",
    "This callback handler illustrates the event loop lifecycle events and how they relate to each other. It's useful for understanding the flow of execution in the Strands agent: \n",
    "\n",
    "**The output will show the sequence of events:**\n",
    "- First the event loop initializes (`init_event_loop`)\n",
    "- Then the cycle begins (`start_event_loop`)\n",
    "- New cycles may start multiple times during execution (`start`)\n",
    "- Text generation and tool usage events occur during the cycle\n",
    "- Finally, the cycle completes (`complete`) or may be force-stopped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b835f-fa62-4e58-80a9-74a27e81a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands_tools import calculator\n",
    "\n",
    "def event_loop_tracker(**kwargs):\n",
    "    # Track event loop lifecycle\n",
    "    if kwargs.get(\"init_event_loop\", False):\n",
    "        print(\"ðŸ”„ Event loop initialized\")\n",
    "    elif kwargs.get(\"start_event_loop\", False):\n",
    "        print(\"â–¶ï¸ Event loop cycle starting\")\n",
    "    elif kwargs.get(\"start\", False):\n",
    "        print(\"ðŸ“ New cycle started\")\n",
    "    elif \"message\" in kwargs:\n",
    "        print(f\"ðŸ“¬ New message created: {kwargs['message']['role']}\")\n",
    "    elif kwargs.get(\"complete\", False):\n",
    "        print(\"âœ… Cycle completed\")\n",
    "    elif kwargs.get(\"force_stop\", False):\n",
    "        print(f\"ðŸ›‘ Event loop force-stopped: {kwargs.get('force_stop_reason', 'unknown reason')}\")\n",
    "\n",
    "    # Track tool usage\n",
    "    if \"current_tool_use\" in kwargs and kwargs[\"current_tool_use\"].get(\"name\"):\n",
    "        tool_name = kwargs[\"current_tool_use\"][\"name\"]\n",
    "        print(f\"ðŸ”§ Using tool: {tool_name}\")\n",
    "\n",
    "    # Show only a snippet of text to keep output clean\n",
    "    if \"data\" in kwargs:\n",
    "        # Only show first 20 chars of each chunk for demo purposes\n",
    "        data_snippet = kwargs[\"data\"][:20] + (\"...\" if len(kwargs[\"data\"]) > 20 else \"\")\n",
    "        print(f\"ðŸ“Ÿ Text: {data_snippet}\")\n",
    "\n",
    "# Create agent with event loop tracker\n",
    "agent = Agent(\n",
    "    tools=[calculator],\n",
    "    callback_handler=event_loop_tracker\n",
    ")\n",
    "\n",
    "# This will show the full event lifecycle in the console\n",
    "callback_handler_agent_response = agent(\"What is the capital of France and what is 42+7?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b75cafe-dbbd-47b3-8515-6320869d6ff1",
   "metadata": {},
   "source": [
    "### **1.h Observability**\n",
    "\n",
    "In the Strands Agents SDK, observability refers to the ability to measure system behavior and performance. [Observability](https://strandsagents.com/latest/user-guide/observability-evaluation/observability/) is the combination of instrumentation, data collection, and analysis techniques that provide insights into an agent's behavior and performance.\n",
    "Building observable agents starts with monitoring the right telemetry. While we leverage the same fundamental building blocks as traditional software â€” [traces](https://strandsagents.com/latest/user-guide/observability-evaluation/traces/), [metrics](https://strandsagents.com/latest/user-guide/observability-evaluation/metrics/), and [logs](https://strandsagents.com/latest/user-guide/observability-evaluation/logs/) â€” their application to agents requires special consideration. We need to capture not only standard application telemetry but also AI-specific signals like model interactions, reasoning steps, and tool usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d18da5-c6da-4994-9a91-37c5936697b4",
   "metadata": {},
   "source": [
    "### **[Metrics](https://strandsagents.com/latest/user-guide/observability-evaluation/metrics/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28773158-cd30-4916-8957-741e8819e0b9",
   "metadata": {},
   "source": [
    "#### **Metrics for the agent invocation using non-default model (Metrics for Step 1.c)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef85b784-79a0-44a7-85d8-f7c0ea93cb3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%store -r agent_response_with_nova_premier\n",
    "agent_response_with_nova_premier.metrics.get_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d538d-0049-434a-b8f2-f25b9162c6f9",
   "metadata": {},
   "source": [
    "#### **Metrics for the agent invocation using callback handler (Metrics for Step 1.g)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a36f5-6a64-4d03-9983-872c90747e6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "callback_handler_agent_response.metrics.get_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774d4e48-833d-4c61-8a3c-122064e85538",
   "metadata": {},
   "source": [
    "### **[Traces](https://strandsagents.com/latest/user-guide/observability-evaluation/traces/)**\n",
    "\n",
    "Tracing is a fundamental component of the Strands SDK's observability framework, providing detailed insights into your agent's execution. Using the OpenTelemetry standard, Strands traces capture the complete journey of a request through your agent, including LLM interactions, retrievers, tool usage, and event loop processing.\n",
    "\n",
    "Strands natively integrates with OpenTelemetry, an industry standard for distributed tracing.\n",
    "\n",
    "----\n",
    "\n",
    "#### **Trace Structure**\n",
    "Strands creates a hierarchical trace structure that mirrors the execution of your agent: \n",
    "- **Agent Span:** The top-level span representing the entire agent invocation - Contains overall metrics like total token usage and cycle count - Captures the user prompt and final response\n",
    "- **Cycle Spans:** Child spans for each event loop cycle - Tracks the progression of thought and reasoning - Shows the transformation from prompt to response\n",
    "- **LLM Spans:** Model invocation spans - Contains prompt, completion, and token usage - Includes model-specific parameters\n",
    "- **Tool Spans:** Tool execution spans - Captures tool name, parameters, and results - Measures tool execution time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8316a981-b924-4ef8-b971-5cb3c0d037f3",
   "metadata": {},
   "source": [
    "Below example shows the detailed traces that can be sent to any of the OTEL compatible tool for visualization and anaysis   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d3ca4-3df8-4974-b4aa-4a3368a39e29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Option 3: Use StrandsTelemetry with your own tracer provider\n",
    "# (Keeps your tracer provider, adds Strands exporters without setting global)\n",
    "from strands.telemetry import StrandsTelemetry\n",
    "\n",
    "strands_telemetry = StrandsTelemetry()\n",
    "strands_telemetry.setup_otlp_exporter().setup_console_exporter()  # Chaining supported\n",
    "\n",
    "# Create agent (tracing will be enabled automatically)\n",
    "agent = Agent(\n",
    "    model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    system_prompt=\"You are a helpful AI assistant\"\n",
    ")\n",
    "\n",
    "# Use agent normally\n",
    "response = agent(\"What can you help me with?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ad668-f820-4c66-a64a-7363b0853db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable this section if you have an LangFuse endpoint to deliver the traces\n",
    "#import os\n",
    "#import base64\n",
    "\n",
    "#os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"\"\n",
    " \n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "#os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-.....\"\n",
    "#os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-.....\" \n",
    "#os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # ðŸ‡ªðŸ‡º EU region (default)\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ðŸ‡ºðŸ‡¸ US region\n",
    " \n",
    "# Build Basic Auth header.\n",
    "#LANGFUSE_AUTH = base64.b64encode(\n",
    "#    f\"{os.environ.get('LANGFUSE_PUBLIC_KEY')}:{os.environ.get('LANGFUSE_SECRET_KEY')}\".encode()\n",
    "#).decode()\n",
    " \n",
    "# Configure OpenTelemetry endpoint & headers\n",
    "#os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = os.environ.get(\"LANGFUSE_HOST\") + \"/api/public/otel/v1/traces\"\n",
    "#os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b340a73-b7f6-4fd6-933e-dba810f058f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from strands.telemetry import StrandsTelemetry\n",
    "\n",
    "\n",
    "strands_telemetry = StrandsTelemetry()\n",
    "strands_telemetry.setup_console_exporter()   # Print traces to console\n",
    "\n",
    "# Uncomment this statement if the OTLP endpoint is configured in the previous cell\n",
    "#strands_telemetry.setup_otlp_exporter()      # Send traces to OTLP endpoint\n",
    "\n",
    "# Create agent\n",
    "agent = Agent(\n",
    "    model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    system_prompt=\"You are a helpful AI assistant\",\n",
    "    trace_attributes={\n",
    "        \"session.id\": \"test-session-1234\", # Example session ID\n",
    "        \"user.id\": \"rs-ws@demo.com\", # Example user ID\n",
    "        \"langfuse.tags\": [\n",
    "            \"Agent-SDK-Example\",\n",
    "            \"Strands-Project-Demo\",\n",
    "            \"Observability-Tutorial\"\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Execute a series of interactions that will be traced\n",
    "response = agent(\"Find me information about Mars. What is its atmosphere like?\")\n",
    "print(response)\n",
    "\n",
    "# Ask a follow-up that uses tools\n",
    "response = agent(\"Calculate how long it would take to travel from Earth to Mars at 100,000 km/h\")\n",
    "print(response)\n",
    "\n",
    "# Each interaction creates a complete trace that can be visualized in your tracing tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59661768-dbad-4489-9307-8cdbd94d028c",
   "metadata": {},
   "source": [
    "### **1.h Agent State Management**\n",
    "\n",
    "Strands [Agents state](https://strandsagents.com/latest/user-guide/concepts/agents/state-sessions/) is maintained in several forms:\n",
    "- **Conversation History:** The sequence of messages between the user and the agent.\n",
    "- **Agent State:** Stateful information outside of conversation context, maintained across multiple requests.\n",
    "- **Request State:** Contextual information maintained within a single request.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4c4eaf-4600-4a18-953b-99fdcd912f17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T03:07:10.464244Z",
     "iopub.status.busy": "2025-07-05T03:07:10.463936Z",
     "iopub.status.idle": "2025-07-05T03:07:10.468667Z",
     "shell.execute_reply": "2025-07-05T03:07:10.467719Z",
     "shell.execute_reply.started": "2025-07-05T03:07:10.464223Z"
    }
   },
   "source": [
    "#### **Conversation History**\n",
    "\n",
    "Conversation history is the primary form of context in a Strands agent, directly accessible through the `agent.messages` property. \n",
    "\n",
    "**Below example shows how the conversation gets updated after each iteration of agent invocation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcef9e12-c85d-4960-a08b-3c68bab2ce5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an agent with initial messages\n",
    "agent = Agent(messages=[\n",
    "    {\"role\": \"user\", \"content\": [{\"text\": \"Hello, my name is Strands!\"}]},\n",
    "    {\"role\": \"assistant\", \"content\": [{\"text\": \"Hi there! How can I help you today?\"}]}\n",
    "])\n",
    "# Access the conversation history\n",
    "print (\"Initial Conversation :: \", agent.messages)\n",
    "print (\"\\n\")\n",
    "\n",
    "agent(\"Explain Agentic AI under 100 words?\")\n",
    "print (\"\\n\")\n",
    "print (\"Next Iteration :: \", agent.messages)\n",
    "print (\"\\n\")\n",
    "\n",
    "agent(\"Describe Amazon Bedrock under 100 words?\")\n",
    "print (\"\\n\")\n",
    "print(\"Final Iteration :: \", agent.messages)  # Shows all messages exchanged so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56006838-248c-4981-b4f4-64207921c548",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T03:15:07.438475Z",
     "iopub.status.busy": "2025-07-05T03:15:07.438125Z",
     "iopub.status.idle": "2025-07-05T03:15:07.442203Z",
     "shell.execute_reply": "2025-07-05T03:15:07.441446Z",
     "shell.execute_reply.started": "2025-07-05T03:15:07.438451Z"
    }
   },
   "source": [
    "### **Conversation Manager**\n",
    "\n",
    "Strands uses a **[conversation manager](https://strandsagents.com/latest/user-guide/concepts/agents/context-management/)** to handle conversation history effectively. The default is the [SlidingWindowConversationManager](https://strandsagents.com/latest/api-reference/agent/#strands.agent.conversation_manager.sliding_window_conversation_manager.SlidingWindowConversationManager), which keeps recent messages and removes older ones when needed. This is the **default conversation manager** used by the Agent class.\n",
    "\n",
    "Strands also supports [NullConversationManager](https://strandsagents.com/latest/api-reference/agent/#strands.agent.conversation_manager.null_conversation_manager.NullConversationManager), which is a simple implementation that does not modify the conversation history.\n",
    "\n",
    "The third is [SummarizingConversationManager](https://strandsagents.com/latest/api-reference/agent/#strands.agent.conversation_manager.summarizing_conversation_manager.SummarizingConversationManager), which implements intelligent conversation context management by summarizing older messages instead of simply discarding them. This approach preserves important information while staying within context limits. By default, the `SummarizingConversationManager` uses the same model and configuration as the main agent to perform summarization.\n",
    "\n",
    "Refer the [context-management documentation](https://strandsagents.com/latest/user-guide/concepts/agents/context-management/), for more details on these 3 options. We will explore an example of `SummarizingConversationManager` in the next cell.\n",
    "\n",
    "\n",
    "##### **Configuration parameters:**\n",
    "- **`summary_ratio` (float, default: 0.3):** Percentage of messages to summarize when reducing context (clamped between 0.1 and 0.8)\n",
    "- **`preserve_recent_messages` (int, default: 10):** Minimum number of recent messages to always keep\n",
    "- **`summarization_agent` (Agent, optional):** Custom agent for generating summaries. If not provided, uses the main agent instance. Cannot be used together with summarization_system_prompt.\n",
    "- **`summarization_system_prompt` (str, optional):** Custom system prompt for summarization. If not provided, uses a default prompt that creates structured bullet-point summaries focusing on key topics, tools used, and technical information in third-person format. Cannot be used together with summarization_agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac157d0-8e05-4d09-8e22-a213b2347610",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T08:30:20.031480Z",
     "iopub.status.busy": "2025-07-05T08:30:20.031169Z",
     "iopub.status.idle": "2025-07-05T08:30:20.036101Z",
     "shell.execute_reply": "2025-07-05T08:30:20.035239Z",
     "shell.execute_reply.started": "2025-07-05T08:30:20.031457Z"
    }
   },
   "source": [
    "<div style=\"background-color: #f44336; color: white; padding: 15px; border-radius: 5px; margin: 10px 0;width: 85%;\">\n",
    "  <h2><strong> Work In Progress </strong><br></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a3db1f-c7c4-49dd-b2b6-ea91b99ba657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands.agent.conversation_manager import SummarizingConversationManager\n",
    "\n",
    "# Create a cheaper, faster model for summarization tasks\n",
    "summarization_model = BedrockModel(\n",
    "    model_id=\"anthropic.claude-3-5-haiku-20241022-v1:0\",  # More cost-effective for summarization\n",
    "    max_tokens=100,\n",
    "    params={\"temperature\": 0.1}  # Low temperature for consistent summaries\n",
    ")\n",
    "\n",
    "custom_summarization_agent = Agent(model=summarization_model)\n",
    "\n",
    "# Custom system prompt for technical conversations\n",
    "custom_system_prompt = \"\"\"\n",
    "You are summarizing a conversation. Create a concise summary that is under 200 words\"\"\"\n",
    "\n",
    "conversation_manager = SummarizingConversationManager(\n",
    "    summary_ratio=0.6,\n",
    "    preserve_recent_messages=1,\n",
    "    summarization_system_prompt=custom_system_prompt\n",
    ")\n",
    "\n",
    "agent = Agent(conversation_manager=conversation_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb3ab69-636f-4d6c-8f3c-5cd855563967",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"Tell me about Generative AI and responsible AI in detail\")\n",
    "print (\"\\n\")\n",
    "# Access the conversation history\n",
    "print (\"Initial Iteration :: \", agent.messages)\n",
    "print (\"\\n\")\n",
    "\n",
    "agent(\"Tell me about Agentic AI in detail\")\n",
    "print (\"\\n\")\n",
    "print (\"Second Iteration :: \", agent.messages)\n",
    "print (\"\\n\")\n",
    "\n",
    "agent(\"Tell me about Amazon Bedrock in detail\")\n",
    "print (\"\\n\")\n",
    "print (\"Next Iteration :: \", agent.messages)\n",
    "print (\"\\n\")\n",
    "\n",
    "agent(\"what can you help me with ?\")\n",
    "print (\"\\n\")\n",
    "print(\"Final Iteration :: \", agent.messages)  # Shows all messages exchanged so far\n",
    "print (\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402c5ce-ef4d-4e2f-9edb-9cef0f5c7193",
   "metadata": {},
   "source": [
    "### **1.h Model Context Protocol (MCP) Tools**\n",
    "\n",
    "The **[Model Context Protocol (MCP)](https://modelcontextprotocol.io/)** is an open protocol that standardizes how applications provide context to Large Language Models (LLMs). Strands Agents integrates with MCP to extend agent capabilities through external tools and services.\n",
    "\n",
    "MCP enables communication between agents and MCP servers that provide additional tools. Strands includes built-in support for connecting to MCP servers and using their tools.\n",
    "\n",
    "When working with MCP tools in Strands, all agent operations must be performed within the MCP client's context manager (using a with statement). This requirement ensures that the MCP session remains active and connected while the agent is using the tools. If you attempt to use an agent or its MCP tools outside of this context, you'll encounter errors because the MCP session will have closed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ff7cb-ab48-442f-b050-008f4062f069",
   "metadata": {},
   "source": [
    "#### **Streamable HTTP**\n",
    "\n",
    "For HTTP-based MCP servers that use Streamable-HTTP Events transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268fade2-1b84-40ab-b457-f31d584c1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "from strands import Agent\n",
    "from strands.tools.mcp.mcp_client import MCPClient\n",
    "\n",
    "streamable_http_mcp_client = MCPClient(lambda: streamablehttp_client(\"http://localhost:8000/mcp\"))\n",
    "\n",
    "# Create an agent with MCP tools\n",
    "with streamable_http_mcp_client:\n",
    "    # Get the tools from the MCP server\n",
    "    tools = streamable_http_mcp_client.list_tools_sync()\n",
    "\n",
    "    # Create an agent with these tools\n",
    "    agent = Agent(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880e4201-c3f0-4a96-9cc2-78047d3eaf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88afd92f-a2b9-49e7-b268-2a77030e3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Upcoming Sections -\n",
    "\n",
    "- Simple Swarm agent\n",
    "- A2A\n",
    "- Agent as a tool\n",
    "- Agent Evaluations\n",
    "- Deploy to Lambda\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47610d6",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "You have successfully completed Lab 0. Now let's move on to the next lab. Open `lab 1a\\faq-agent` to continue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
